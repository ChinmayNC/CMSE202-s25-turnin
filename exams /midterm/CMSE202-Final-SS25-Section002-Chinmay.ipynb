{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: right;\"> &#9989; Chinmay Chouthai </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSE 202 Final (Section 002 - Spring 2025) (80 points total)\n",
    "\n",
    "The goal of this final is to give you the opportunity to test out some of the skills that you've developed having now finished CMSE 202. In particular, you'll be committing and pushing repository changes to a GitHub repository, working with data to build a network graph, performing regression analysis, and classifying data using a machine learning classifier. You should find that you have all of the skills necessary to complete this exam having completed the second half of CMSE 202!\n",
    "\n",
    "You are encouraged to look through the entire exam before you get started so that you can appropriately budget your time and understand the broad goals of the exam. Once you've read through it, you'll probably want to make sure you do Part 1 first to ensure that your GitHub repository is working correctly. Let your instructor know right away if you run into issues!\n",
    "\n",
    "**Important note about using online resources**: This exam is \"open internet\". That means that you can look up documentation, google how to accomplish certain Python tasks, etc. Being able to effectively use the internet for computational modeling and data science is a very important skill, so we want to make sure you have the opportunity to exercise that skill. You can also use _your version_ of past CMSE 202 assignments and the CMSE 202 course materials as a resource! **However: The use of any person-to-person communication software or is absolutely not acceptable.** If you are seen accessing your email, using a collaborative cloud storage or document software (e.g. Slack, Google Documents) you will be at risk for receiving a zero on the exam. Regardless of what resources you use whether they are ai or stackexchange, CITE THEM PLEASE. You will get marked down if you provide a response that looks copy and pasted without a citation. If you are unsure of how to do a citation or if your citation is good enough, ask your professor or instructional staff for help.\n",
    "\n",
    "**Keep your eyes on your screen!** Unfortunately, there isn't enough space in the room for everyone to sit at their own table so please do your best to keep your eyes on your own screen. This exam is designed to give *you* the opportunity to show the instructor what you can do and you should hold yourself accountable for maintaining a high level of academic integrity. If any of the instructors observe suspicious behavior, you will, again, risk receiving a zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 0: Academic integrity statement\n",
    "\n",
    "Read the following statement and edit the markdown text to put your name in the statement. This is your commitment to doing your own authentic work on this exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I, **Chinmay Chouthai**, affirm that this exam represents my own authetic work, without the use of any unpermitted aids or resources or person-to-person communication. I understand that this exam an an opportunity to showcase my own progress in developing and improving my computational skills and have done my best to demonstrate those skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 1: Add to your Git repository to track your progress on your exam (7 points)\n",
    "\n",
    "Before you get to far along in the exam, you're going to add it to the `cmse202-s25-turnin` repository you created in class so that you can track your progress on the exam and preserve the final version that you turn in. In order to do this you need to\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "1. Navigate to your `cmse202-s25-turnin` repository and create a new directory called `final`. You may have already done this in the midterm. If you have, then good job move on to the next part!\n",
    "2. Move this notebook into that **new directory** in your repository, then **add it and commit it to your repository**.\n",
    "1. Finally, to test that everything is working, \"git push\" the file so that it ends up in your GitHub repository.\n",
    "\n",
    "**Important**: Double check you've added your Professor and your TA as collaborators to your \"turnin\" respository (you should have done this previously).\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the noteobok, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-s25-turnin`\" repository inside the `final` directory that you just created. Periodically, **you'll be asked to commit your changes to the repository and push them to the remote GitHub location**. Of course, you can always commit your changes more often than that, if you wish.  It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit.\n",
    "\n",
    "&#9989; **Do this**: Before you move on, put the command that your instructor should run to clone your repository in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "git clone url \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 2: Graph Theory and Networks (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.1 (9 points)**: We want you to make a very basic model of the personal network you have built this term. Using Networkx, visualize a network that has at least five (including yourself) CMSE202 classmates you have met and worked with this term. There should be an edge connecting you to all of the other classmates. Add at least one edge between two classmates that know each other (besides yourself). If you do not know names, use your group project member names to make this graph. The Day 10 ICA and PCA should help with doing this. I reccomend using dictionaries to make your network. See if you can display the names of this network on your diagram as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Put your code for Question 2.1 here\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "G = nx.Graph()\n",
    "people = ['Aiden', 'Zack', 'Colin', 'Chinmay', 'Jacob']\n",
    "G.add_nodes_from(people)\n",
    "friends = ['Aiden', 'Zack', 'Colin', 'Chinmay']\n",
    "for i in range(len(friends)):\n",
    "    for j in range(i + 1, len(friends)):\n",
    "        G.add_edge(friends[i], friends[j])\n",
    "G.add_edge('Chinmay', 'Jacob')\n",
    "pos = nx.spring_layout(G, seed=42)  \n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_size=12)\n",
    "plt.title('Class Connections')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 2**\", and push the changes to GitHub.\n",
    "\n",
    "\n",
    "If committing and/or pushing isn't working for you, write down the complete commands in this cell that would have committed your changes (with the commit message) and pushed them to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 3: Regression Analysis on Data (27 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this part is to do a multivariate linear regression, also known as multiple linear regression, to understand the relation between renting a bike and other factors. You can download the dataset from this URL:\n",
    "\n",
    "`https://raw.githubusercontent.com/gambre11/CMSE202/refs/heads/main/day.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.1 (3 points)**: To get started, **download the `day.csv` file and place it in the same directory as your notebook**, then **read in the `day.csv` dataset and name it `bike_data`** and finally **display the first few rows of the data**. You can use `Pandas` for this task or any other Python tool you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.204348</td>\n",
       "      <td>0.233209</td>\n",
       "      <td>0.518261</td>\n",
       "      <td>0.089565</td>\n",
       "      <td>88</td>\n",
       "      <td>1518</td>\n",
       "      <td>1606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.196522</td>\n",
       "      <td>0.208839</td>\n",
       "      <td>0.498696</td>\n",
       "      <td>0.168726</td>\n",
       "      <td>148</td>\n",
       "      <td>1362</td>\n",
       "      <td>1510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2011-01-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.162254</td>\n",
       "      <td>0.535833</td>\n",
       "      <td>0.266804</td>\n",
       "      <td>68</td>\n",
       "      <td>891</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2011-01-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.116175</td>\n",
       "      <td>0.434167</td>\n",
       "      <td>0.361950</td>\n",
       "      <td>54</td>\n",
       "      <td>768</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2011-01-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150833</td>\n",
       "      <td>0.150888</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>0.223267</td>\n",
       "      <td>41</td>\n",
       "      <td>1280</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1        0        6           0   \n",
       "1        2  2011-01-02       1   0     1        0        0           0   \n",
       "2        3  2011-01-03       1   0     1        0        1           1   \n",
       "3        4  2011-01-04       1   0     1        0        2           1   \n",
       "4        5  2011-01-05       1   0     1        0        3           1   \n",
       "5        6  2011-01-06       1   0     1        0        4           1   \n",
       "6        7  2011-01-07       1   0     1        0        5           1   \n",
       "7        8  2011-01-08       1   0     1        0        6           0   \n",
       "8        9  2011-01-09       1   0     1        0        0           0   \n",
       "9       10  2011-01-10       1   0     1        0        1           1   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "5           1  0.204348  0.233209  0.518261   0.089565      88        1518   \n",
       "6           2  0.196522  0.208839  0.498696   0.168726     148        1362   \n",
       "7           2  0.165000  0.162254  0.535833   0.266804      68         891   \n",
       "8           1  0.138333  0.116175  0.434167   0.361950      54         768   \n",
       "9           1  0.150833  0.150888  0.482917   0.223267      41        1280   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  \n",
       "5  1606  \n",
       "6  1510  \n",
       "7   959  \n",
       "8   822  \n",
       "9  1321  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here \n",
    "import pandas as pd \n",
    "bike_data= pd.read_csv('day.csv')\n",
    "bike_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information for this dataset can be found here: https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset/data\n",
    "\n",
    "You will be trying to predict `cnt` (which is the number of bike rentals) using linear regression with the features.\n",
    "\n",
    "\n",
    "&#9989; **Question 3.2 (2 points)**: Remove the columns `dteday`, `instant`,`casual`, and `registered` from the data you loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here \n",
    "bike_new=bike_data.drop(['dteday', 'instant','casual','registered'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; Run this following code cell. This is taking some columns and separating the data. You will likely need to change the variable name to match the variable name of your data set though so pay attention to everywhere it mentions `bike_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "bike_new['season']=bike_new['season'].astype('category')\n",
    "bike_new['weathersit']=bike_new['weathersit'].astype('category')\n",
    "bike_new['mnth']=bike_new['mnth'].astype('category')\n",
    "bike_new['weekday']=bike_new['weekday'].astype('category')\n",
    "\n",
    "bike_new = pd.get_dummies(bike_new, drop_first=True)\n",
    "bike_new.replace({False: 0, True: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.3 (3 points)**: **Construct two data frames** using the loaded and cleaned data: one named `labels` and the other named `features`. The `labels` data frame should consist solely of the `cnt` column, while the `features` data frame should contain all the other columns. **Display the first few lines of these data frames.** Note the pandas `.pop` method may be helpful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>...</th>\n",
       "      <th>mnth_11</th>\n",
       "      <th>mnth_12</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>weathersit_2</th>\n",
       "      <th>weathersit_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.204348</td>\n",
       "      <td>0.233209</td>\n",
       "      <td>0.518261</td>\n",
       "      <td>0.089565</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196522</td>\n",
       "      <td>0.208839</td>\n",
       "      <td>0.498696</td>\n",
       "      <td>0.168726</td>\n",
       "      <td>1510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.162254</td>\n",
       "      <td>0.535833</td>\n",
       "      <td>0.266804</td>\n",
       "      <td>959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.116175</td>\n",
       "      <td>0.434167</td>\n",
       "      <td>0.361950</td>\n",
       "      <td>822</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150833</td>\n",
       "      <td>0.150888</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>0.223267</td>\n",
       "      <td>1321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   yr  holiday  workingday      temp     atemp       hum  windspeed   cnt  \\\n",
       "0   0        0           0  0.344167  0.363625  0.805833   0.160446   985   \n",
       "1   0        0           0  0.363478  0.353739  0.696087   0.248539   801   \n",
       "2   0        0           1  0.196364  0.189405  0.437273   0.248309  1349   \n",
       "3   0        0           1  0.200000  0.212122  0.590435   0.160296  1562   \n",
       "4   0        0           1  0.226957  0.229270  0.436957   0.186900  1600   \n",
       "5   0        0           1  0.204348  0.233209  0.518261   0.089565  1606   \n",
       "6   0        0           1  0.196522  0.208839  0.498696   0.168726  1510   \n",
       "7   0        0           0  0.165000  0.162254  0.535833   0.266804   959   \n",
       "8   0        0           0  0.138333  0.116175  0.434167   0.361950   822   \n",
       "9   0        0           1  0.150833  0.150888  0.482917   0.223267  1321   \n",
       "\n",
       "   season_2  season_3  ...  mnth_11  mnth_12  weekday_1  weekday_2  weekday_3  \\\n",
       "0         0         0  ...        0        0          0          0          0   \n",
       "1         0         0  ...        0        0          0          0          0   \n",
       "2         0         0  ...        0        0          1          0          0   \n",
       "3         0         0  ...        0        0          0          1          0   \n",
       "4         0         0  ...        0        0          0          0          1   \n",
       "5         0         0  ...        0        0          0          0          0   \n",
       "6         0         0  ...        0        0          0          0          0   \n",
       "7         0         0  ...        0        0          0          0          0   \n",
       "8         0         0  ...        0        0          0          0          0   \n",
       "9         0         0  ...        0        0          1          0          0   \n",
       "\n",
       "   weekday_4  weekday_5  weekday_6  weathersit_2  weathersit_3  \n",
       "0          0          0          1             1             0  \n",
       "1          0          0          0             1             0  \n",
       "2          0          0          0             0             0  \n",
       "3          0          0          0             0             0  \n",
       "4          0          0          0             0             0  \n",
       "5          1          0          0             0             0  \n",
       "6          0          1          0             1             0  \n",
       "7          0          0          1             1             0  \n",
       "8          0          0          0             0             0  \n",
       "9          0          0          0             0             0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "labels = bike_data[\"cnt\"]\n",
    "features = bike_new\n",
    "labels.head (10)\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fit the data using the ordinary least squares model `OLS` in `statsmodel`. \n",
    "\n",
    "&#9989; **Question 3.4 (2 point)**: Before proceeding, **add a column of constants** (set to 1.0) to the `features` data frame. You learned about a `statsmodels` function in class that can accomplish this task. Label the modified data frame as `features_const`. **Display** `features_const` to verify that the new column is indeed added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>yr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_2</th>\n",
       "      <th>...</th>\n",
       "      <th>mnth_11</th>\n",
       "      <th>mnth_12</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>weathersit_2</th>\n",
       "      <th>weathersit_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.204348</td>\n",
       "      <td>0.233209</td>\n",
       "      <td>0.518261</td>\n",
       "      <td>0.089565</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196522</td>\n",
       "      <td>0.208839</td>\n",
       "      <td>0.498696</td>\n",
       "      <td>0.168726</td>\n",
       "      <td>1510</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.162254</td>\n",
       "      <td>0.535833</td>\n",
       "      <td>0.266804</td>\n",
       "      <td>959</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.116175</td>\n",
       "      <td>0.434167</td>\n",
       "      <td>0.361950</td>\n",
       "      <td>822</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150833</td>\n",
       "      <td>0.150888</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>0.223267</td>\n",
       "      <td>1321</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   const  yr  holiday  workingday      temp     atemp       hum  windspeed  \\\n",
       "0    1.0   0        0           0  0.344167  0.363625  0.805833   0.160446   \n",
       "1    1.0   0        0           0  0.363478  0.353739  0.696087   0.248539   \n",
       "2    1.0   0        0           1  0.196364  0.189405  0.437273   0.248309   \n",
       "3    1.0   0        0           1  0.200000  0.212122  0.590435   0.160296   \n",
       "4    1.0   0        0           1  0.226957  0.229270  0.436957   0.186900   \n",
       "5    1.0   0        0           1  0.204348  0.233209  0.518261   0.089565   \n",
       "6    1.0   0        0           1  0.196522  0.208839  0.498696   0.168726   \n",
       "7    1.0   0        0           0  0.165000  0.162254  0.535833   0.266804   \n",
       "8    1.0   0        0           0  0.138333  0.116175  0.434167   0.361950   \n",
       "9    1.0   0        0           1  0.150833  0.150888  0.482917   0.223267   \n",
       "\n",
       "    cnt  season_2  ...  mnth_11  mnth_12  weekday_1  weekday_2  weekday_3  \\\n",
       "0   985         0  ...        0        0          0          0          0   \n",
       "1   801         0  ...        0        0          0          0          0   \n",
       "2  1349         0  ...        0        0          1          0          0   \n",
       "3  1562         0  ...        0        0          0          1          0   \n",
       "4  1600         0  ...        0        0          0          0          1   \n",
       "5  1606         0  ...        0        0          0          0          0   \n",
       "6  1510         0  ...        0        0          0          0          0   \n",
       "7   959         0  ...        0        0          0          0          0   \n",
       "8   822         0  ...        0        0          0          0          0   \n",
       "9  1321         0  ...        0        0          1          0          0   \n",
       "\n",
       "   weekday_4  weekday_5  weekday_6  weathersit_2  weathersit_3  \n",
       "0          0          0          1             1             0  \n",
       "1          0          0          0             1             0  \n",
       "2          0          0          0             0             0  \n",
       "3          0          0          0             0             0  \n",
       "4          0          0          0             0             0  \n",
       "5          1          0          0             0             0  \n",
       "6          0          1          0             1             0  \n",
       "7          0          0          1             1             0  \n",
       "8          0          0          0             0             0  \n",
       "9          0          0          0             0             0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here \n",
    "import statsmodels.api as sm \n",
    "features_const=sm.add_constant(features) \n",
    "features_const.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will perform the actual fit.\n",
    "\n",
    "&#9989; **Question 3.5 (4 points)**: Using `statsmodels` `OLS`, **perform a fit** using `labels` (containing `cnt`) as the dependent variable and `features_const` as the independent variable. **Print the fit** using `summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>cnt</td>       <th>  R-squared:         </th>  <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>2.936e+30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 30 Apr 2025</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:12:03</td>     <th>  Log-Likelihood:    </th>  <td>  17908.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   731</td>      <th>  AIC:               </th> <td>-3.576e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   701</td>      <th>  BIC:               </th> <td>-3.562e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    29</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td>-4.576e-12</td> <td> 1.82e-12</td> <td>   -2.520</td> <td> 0.012</td> <td>-8.14e-12</td> <td>-1.01e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr</th>           <td> 7.702e-12</td> <td> 7.07e-13</td> <td>   10.890</td> <td> 0.000</td> <td> 6.31e-12</td> <td> 9.09e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>holiday</th>      <td>-3.062e-12</td> <td> 1.19e-12</td> <td>   -2.576</td> <td> 0.010</td> <td> -5.4e-12</td> <td>-7.28e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>workingday</th>   <td> 9.783e-13</td> <td> 5.01e-13</td> <td>    1.952</td> <td> 0.051</td> <td>-5.88e-15</td> <td> 1.96e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>temp</th>         <td>-3.297e-12</td> <td> 1.03e-11</td> <td>   -0.319</td> <td> 0.750</td> <td>-2.36e-11</td> <td>  1.7e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>atemp</th>        <td>-5.684e-13</td> <td> 1.08e-11</td> <td>   -0.053</td> <td> 0.958</td> <td>-2.18e-11</td> <td> 2.06e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hum</th>          <td>   1.3e-12</td> <td>  2.2e-12</td> <td>    0.591</td> <td> 0.554</td> <td>-3.02e-12</td> <td> 5.62e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>windspeed</th>    <td> 7.674e-13</td> <td> 3.16e-12</td> <td>    0.243</td> <td> 0.808</td> <td>-5.43e-12</td> <td> 6.96e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cnt</th>          <td>    1.0000</td> <td> 2.78e-16</td> <td> 3.59e+15</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_2</th>     <td>-7.958e-13</td> <td> 1.35e-12</td> <td>   -0.591</td> <td> 0.555</td> <td>-3.44e-12</td> <td> 1.85e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_3</th>     <td> 1.762e-12</td> <td> 1.59e-12</td> <td>    1.109</td> <td> 0.268</td> <td>-1.36e-12</td> <td> 4.88e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_4</th>     <td> 2.274e-13</td> <td>  1.4e-12</td> <td>    0.162</td> <td> 0.871</td> <td>-2.53e-12</td> <td> 2.99e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_2</th>       <td>-3.455e-13</td> <td> 1.06e-12</td> <td>   -0.326</td> <td> 0.745</td> <td>-2.43e-12</td> <td> 1.74e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_3</th>       <td>-1.023e-12</td> <td> 1.23e-12</td> <td>   -0.832</td> <td> 0.405</td> <td>-3.44e-12</td> <td> 1.39e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_4</th>       <td>-9.948e-13</td> <td> 1.83e-12</td> <td>   -0.544</td> <td> 0.587</td> <td>-4.59e-12</td> <td>  2.6e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_5</th>       <td>-1.052e-12</td> <td> 1.98e-12</td> <td>   -0.530</td> <td> 0.596</td> <td>-4.95e-12</td> <td> 2.84e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_6</th>       <td>-4.405e-13</td> <td> 2.09e-12</td> <td>   -0.211</td> <td> 0.833</td> <td>-4.54e-12</td> <td> 3.66e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_7</th>       <td>-2.046e-12</td> <td> 2.31e-12</td> <td>   -0.884</td> <td> 0.377</td> <td>-6.59e-12</td> <td>  2.5e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_8</th>       <td>-2.331e-12</td> <td> 2.24e-12</td> <td>   -1.041</td> <td> 0.298</td> <td>-6.73e-12</td> <td> 2.07e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_9</th>       <td>-1.535e-12</td> <td> 1.97e-12</td> <td>   -0.777</td> <td> 0.437</td> <td>-5.41e-12</td> <td> 2.34e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_10</th>      <td>-1.734e-12</td> <td> 1.79e-12</td> <td>   -0.970</td> <td> 0.332</td> <td>-5.24e-12</td> <td> 1.77e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_11</th>      <td>-1.791e-12</td> <td>  1.7e-12</td> <td>   -1.052</td> <td> 0.293</td> <td>-5.13e-12</td> <td> 1.55e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_12</th>      <td>-1.364e-12</td> <td> 1.34e-12</td> <td>   -1.015</td> <td> 0.310</td> <td>   -4e-12</td> <td> 1.27e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_1</th>    <td>-5.702e-13</td> <td> 5.28e-13</td> <td>   -1.080</td> <td> 0.281</td> <td>-1.61e-12</td> <td> 4.67e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_2</th>    <td>-3.908e-13</td> <td>  5.7e-13</td> <td>   -0.685</td> <td> 0.493</td> <td>-1.51e-12</td> <td> 7.29e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_3</th>    <td>-8.242e-13</td> <td> 5.73e-13</td> <td>   -1.439</td> <td> 0.151</td> <td>-1.95e-12</td> <td> 3.01e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_4</th>    <td> -3.57e-13</td> <td> 5.68e-13</td> <td>   -0.628</td> <td> 0.530</td> <td>-1.47e-12</td> <td> 7.58e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_5</th>    <td> 6.146e-13</td> <td> 5.71e-13</td> <td>    1.077</td> <td> 0.282</td> <td>-5.06e-13</td> <td> 1.74e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_6</th>    <td>-1.002e-12</td> <td> 7.95e-13</td> <td>   -1.260</td> <td> 0.208</td> <td>-2.56e-12</td> <td> 5.59e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit_2</th> <td> 1.624e-12</td> <td> 5.83e-13</td> <td>    2.786</td> <td> 0.005</td> <td> 4.79e-13</td> <td> 2.77e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit_3</th> <td> 1.471e-12</td> <td> 1.55e-12</td> <td>    0.947</td> <td> 0.344</td> <td>-1.58e-12</td> <td> 4.52e-12</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4377.552</td> <th>  Durbin-Watson:     </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  77.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.021</td>  <th>  Prob(JB):          </th> <td>1.57e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 1.407</td>  <th>  Cond. No.          </th> <td>1.09e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.48e-22. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       cnt        & \\textbf{  R-squared:         } &     1.000   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     1.000   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } & 2.936e+30   \\\\\n",
       "\\textbf{Date:}             & Wed, 30 Apr 2025 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     16:12:03     & \\textbf{  Log-Likelihood:    } &    17908.   \\\\\n",
       "\\textbf{No. Observations:} &         731      & \\textbf{  AIC:               } & -3.576e+04  \\\\\n",
       "\\textbf{Df Residuals:}     &         701      & \\textbf{  BIC:               } & -3.562e+04  \\\\\n",
       "\\textbf{Df Model:}         &          29      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}         &   -4.576e-12  &     1.82e-12     &    -2.520  &         0.012        &    -8.14e-12    &    -1.01e-12     \\\\\n",
       "\\textbf{yr}            &    7.702e-12  &     7.07e-13     &    10.890  &         0.000        &     6.31e-12    &     9.09e-12     \\\\\n",
       "\\textbf{holiday}       &   -3.062e-12  &     1.19e-12     &    -2.576  &         0.010        &     -5.4e-12    &    -7.28e-13     \\\\\n",
       "\\textbf{workingday}    &    9.783e-13  &     5.01e-13     &     1.952  &         0.051        &    -5.88e-15    &     1.96e-12     \\\\\n",
       "\\textbf{temp}          &   -3.297e-12  &     1.03e-11     &    -0.319  &         0.750        &    -2.36e-11    &      1.7e-11     \\\\\n",
       "\\textbf{atemp}         &   -5.684e-13  &     1.08e-11     &    -0.053  &         0.958        &    -2.18e-11    &     2.06e-11     \\\\\n",
       "\\textbf{hum}           &      1.3e-12  &      2.2e-12     &     0.591  &         0.554        &    -3.02e-12    &     5.62e-12     \\\\\n",
       "\\textbf{windspeed}     &    7.674e-13  &     3.16e-12     &     0.243  &         0.808        &    -5.43e-12    &     6.96e-12     \\\\\n",
       "\\textbf{cnt}           &       1.0000  &     2.78e-16     &  3.59e+15  &         0.000        &        1.000    &        1.000     \\\\\n",
       "\\textbf{season\\_2}     &   -7.958e-13  &     1.35e-12     &    -0.591  &         0.555        &    -3.44e-12    &     1.85e-12     \\\\\n",
       "\\textbf{season\\_3}     &    1.762e-12  &     1.59e-12     &     1.109  &         0.268        &    -1.36e-12    &     4.88e-12     \\\\\n",
       "\\textbf{season\\_4}     &    2.274e-13  &      1.4e-12     &     0.162  &         0.871        &    -2.53e-12    &     2.99e-12     \\\\\n",
       "\\textbf{mnth\\_2}       &   -3.455e-13  &     1.06e-12     &    -0.326  &         0.745        &    -2.43e-12    &     1.74e-12     \\\\\n",
       "\\textbf{mnth\\_3}       &   -1.023e-12  &     1.23e-12     &    -0.832  &         0.405        &    -3.44e-12    &     1.39e-12     \\\\\n",
       "\\textbf{mnth\\_4}       &   -9.948e-13  &     1.83e-12     &    -0.544  &         0.587        &    -4.59e-12    &      2.6e-12     \\\\\n",
       "\\textbf{mnth\\_5}       &   -1.052e-12  &     1.98e-12     &    -0.530  &         0.596        &    -4.95e-12    &     2.84e-12     \\\\\n",
       "\\textbf{mnth\\_6}       &   -4.405e-13  &     2.09e-12     &    -0.211  &         0.833        &    -4.54e-12    &     3.66e-12     \\\\\n",
       "\\textbf{mnth\\_7}       &   -2.046e-12  &     2.31e-12     &    -0.884  &         0.377        &    -6.59e-12    &      2.5e-12     \\\\\n",
       "\\textbf{mnth\\_8}       &   -2.331e-12  &     2.24e-12     &    -1.041  &         0.298        &    -6.73e-12    &     2.07e-12     \\\\\n",
       "\\textbf{mnth\\_9}       &   -1.535e-12  &     1.97e-12     &    -0.777  &         0.437        &    -5.41e-12    &     2.34e-12     \\\\\n",
       "\\textbf{mnth\\_10}      &   -1.734e-12  &     1.79e-12     &    -0.970  &         0.332        &    -5.24e-12    &     1.77e-12     \\\\\n",
       "\\textbf{mnth\\_11}      &   -1.791e-12  &      1.7e-12     &    -1.052  &         0.293        &    -5.13e-12    &     1.55e-12     \\\\\n",
       "\\textbf{mnth\\_12}      &   -1.364e-12  &     1.34e-12     &    -1.015  &         0.310        &       -4e-12    &     1.27e-12     \\\\\n",
       "\\textbf{weekday\\_1}    &   -5.702e-13  &     5.28e-13     &    -1.080  &         0.281        &    -1.61e-12    &     4.67e-13     \\\\\n",
       "\\textbf{weekday\\_2}    &   -3.908e-13  &      5.7e-13     &    -0.685  &         0.493        &    -1.51e-12    &     7.29e-13     \\\\\n",
       "\\textbf{weekday\\_3}    &   -8.242e-13  &     5.73e-13     &    -1.439  &         0.151        &    -1.95e-12    &     3.01e-13     \\\\\n",
       "\\textbf{weekday\\_4}    &    -3.57e-13  &     5.68e-13     &    -0.628  &         0.530        &    -1.47e-12    &     7.58e-13     \\\\\n",
       "\\textbf{weekday\\_5}    &    6.146e-13  &     5.71e-13     &     1.077  &         0.282        &    -5.06e-13    &     1.74e-12     \\\\\n",
       "\\textbf{weekday\\_6}    &   -1.002e-12  &     7.95e-13     &    -1.260  &         0.208        &    -2.56e-12    &     5.59e-13     \\\\\n",
       "\\textbf{weathersit\\_2} &    1.624e-12  &     5.83e-13     &     2.786  &         0.005        &     4.79e-13    &     2.77e-12     \\\\\n",
       "\\textbf{weathersit\\_3} &    1.471e-12  &     1.55e-12     &     0.947  &         0.344        &    -1.58e-12    &     4.52e-12     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 4377.552 & \\textbf{  Durbin-Watson:     } &    0.143  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } &   77.380  \\\\\n",
       "\\textbf{Skew:}          &  -0.021  & \\textbf{  Prob(JB):          } & 1.57e-17  \\\\\n",
       "\\textbf{Kurtosis:}      &   1.407  & \\textbf{  Cond. No.          } & 1.09e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 1.48e-22. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    cnt   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                  1.000\n",
       "Method:                 Least Squares   F-statistic:                 2.936e+30\n",
       "Date:                Wed, 30 Apr 2025   Prob (F-statistic):               0.00\n",
       "Time:                        16:12:03   Log-Likelihood:                 17908.\n",
       "No. Observations:                 731   AIC:                        -3.576e+04\n",
       "Df Residuals:                     701   BIC:                        -3.562e+04\n",
       "Df Model:                          29                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const        -4.576e-12   1.82e-12     -2.520      0.012   -8.14e-12   -1.01e-12\n",
       "yr            7.702e-12   7.07e-13     10.890      0.000    6.31e-12    9.09e-12\n",
       "holiday      -3.062e-12   1.19e-12     -2.576      0.010    -5.4e-12   -7.28e-13\n",
       "workingday    9.783e-13   5.01e-13      1.952      0.051   -5.88e-15    1.96e-12\n",
       "temp         -3.297e-12   1.03e-11     -0.319      0.750   -2.36e-11     1.7e-11\n",
       "atemp        -5.684e-13   1.08e-11     -0.053      0.958   -2.18e-11    2.06e-11\n",
       "hum             1.3e-12    2.2e-12      0.591      0.554   -3.02e-12    5.62e-12\n",
       "windspeed     7.674e-13   3.16e-12      0.243      0.808   -5.43e-12    6.96e-12\n",
       "cnt              1.0000   2.78e-16   3.59e+15      0.000       1.000       1.000\n",
       "season_2     -7.958e-13   1.35e-12     -0.591      0.555   -3.44e-12    1.85e-12\n",
       "season_3      1.762e-12   1.59e-12      1.109      0.268   -1.36e-12    4.88e-12\n",
       "season_4      2.274e-13    1.4e-12      0.162      0.871   -2.53e-12    2.99e-12\n",
       "mnth_2       -3.455e-13   1.06e-12     -0.326      0.745   -2.43e-12    1.74e-12\n",
       "mnth_3       -1.023e-12   1.23e-12     -0.832      0.405   -3.44e-12    1.39e-12\n",
       "mnth_4       -9.948e-13   1.83e-12     -0.544      0.587   -4.59e-12     2.6e-12\n",
       "mnth_5       -1.052e-12   1.98e-12     -0.530      0.596   -4.95e-12    2.84e-12\n",
       "mnth_6       -4.405e-13   2.09e-12     -0.211      0.833   -4.54e-12    3.66e-12\n",
       "mnth_7       -2.046e-12   2.31e-12     -0.884      0.377   -6.59e-12     2.5e-12\n",
       "mnth_8       -2.331e-12   2.24e-12     -1.041      0.298   -6.73e-12    2.07e-12\n",
       "mnth_9       -1.535e-12   1.97e-12     -0.777      0.437   -5.41e-12    2.34e-12\n",
       "mnth_10      -1.734e-12   1.79e-12     -0.970      0.332   -5.24e-12    1.77e-12\n",
       "mnth_11      -1.791e-12    1.7e-12     -1.052      0.293   -5.13e-12    1.55e-12\n",
       "mnth_12      -1.364e-12   1.34e-12     -1.015      0.310      -4e-12    1.27e-12\n",
       "weekday_1    -5.702e-13   5.28e-13     -1.080      0.281   -1.61e-12    4.67e-13\n",
       "weekday_2    -3.908e-13    5.7e-13     -0.685      0.493   -1.51e-12    7.29e-13\n",
       "weekday_3    -8.242e-13   5.73e-13     -1.439      0.151   -1.95e-12    3.01e-13\n",
       "weekday_4     -3.57e-13   5.68e-13     -0.628      0.530   -1.47e-12    7.58e-13\n",
       "weekday_5     6.146e-13   5.71e-13      1.077      0.282   -5.06e-13    1.74e-12\n",
       "weekday_6    -1.002e-12   7.95e-13     -1.260      0.208   -2.56e-12    5.59e-13\n",
       "weathersit_2  1.624e-12   5.83e-13      2.786      0.005    4.79e-13    2.77e-12\n",
       "weathersit_3  1.471e-12   1.55e-12      0.947      0.344   -1.58e-12    4.52e-12\n",
       "==============================================================================\n",
       "Omnibus:                     4377.552   Durbin-Watson:                   0.143\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               77.380\n",
       "Skew:                          -0.021   Prob(JB):                     1.57e-17\n",
       "Kurtosis:                       1.407   Cond. No.                     1.09e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.48e-22. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "fit=sm.OLS(labels,features_const).fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.5 (2 points)**: Among all the features, is there one or more that are significantly less important than others? justify your answer with a sentence or two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> everything except for weathersit 2 and holiday are not statistically significant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.6 (3 points)**: **Remove the \"least important\" features**, then **fit the model again** and **print the fit** using `summary()` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>cnt</td>       <th>  R-squared (uncentered):</th>      <td>   0.237</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.235</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   113.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 30 Apr 2025</td> <th>  Prob (F-statistic):</th>          <td>1.52e-43</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:20:34</td>     <th>  Log-Likelihood:    </th>          <td> -7150.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   731</td>      <th>  AIC:               </th>          <td>1.430e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   729</td>      <th>  BIC:               </th>          <td>1.431e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit_2</th> <td> 3972.7059</td> <td>  273.818</td> <td>   14.509</td> <td> 0.000</td> <td> 3435.140</td> <td> 4510.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>holiday</th>      <td> 2599.9412</td> <td>  939.076</td> <td>    2.769</td> <td> 0.006</td> <td>  756.325</td> <td> 4443.558</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>52.513</td> <th>  Durbin-Watson:     </th> <td>   0.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  25.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.276</td> <th>  Prob(JB):          </th> <td>2.50e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.263</td> <th>  Cond. No.          </th> <td>    3.44</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       cnt        & \\textbf{  R-squared (uncentered):}      &     0.237   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared (uncentered):} &     0.235   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       }          &     113.2   \\\\\n",
       "\\textbf{Date:}             & Wed, 30 Apr 2025 & \\textbf{  Prob (F-statistic):}          &  1.52e-43   \\\\\n",
       "\\textbf{Time:}             &     16:20:34     & \\textbf{  Log-Likelihood:    }          &   -7150.1   \\\\\n",
       "\\textbf{No. Observations:} &         731      & \\textbf{  AIC:               }          & 1.430e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &         729      & \\textbf{  BIC:               }          & 1.431e+04   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     }          &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     }          &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{weathersit\\_2} &    3972.7059  &      273.818     &    14.509  &         0.000        &     3435.140    &     4510.272     \\\\\n",
       "\\textbf{holiday}       &    2599.9412  &      939.076     &     2.769  &         0.006        &      756.325    &     4443.558     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 52.513 & \\textbf{  Durbin-Watson:     } &    0.472  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.000 & \\textbf{  Jarque-Bera (JB):  } &   25.800  \\\\\n",
       "\\textbf{Skew:}          & -0.276 & \\textbf{  Prob(JB):          } & 2.50e-06  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.263 & \\textbf{  Cond. No.          } &     3.44  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] R is computed without centering (uncentered) since the model does not contain a constant. \\newline\n",
       " [2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                    cnt   R-squared (uncentered):                   0.237\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.235\n",
       "Method:                 Least Squares   F-statistic:                              113.2\n",
       "Date:                Wed, 30 Apr 2025   Prob (F-statistic):                    1.52e-43\n",
       "Time:                        16:20:34   Log-Likelihood:                         -7150.1\n",
       "No. Observations:                 731   AIC:                                  1.430e+04\n",
       "Df Residuals:                     729   BIC:                                  1.431e+04\n",
       "Df Model:                           2                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "weathersit_2  3972.7059    273.818     14.509      0.000    3435.140    4510.272\n",
       "holiday       2599.9412    939.076      2.769      0.006     756.325    4443.558\n",
       "==============================================================================\n",
       "Omnibus:                       52.513   Durbin-Watson:                   0.472\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               25.800\n",
       "Skew:                          -0.276   Prob(JB):                     2.50e-06\n",
       "Kurtosis:                       2.263   Cond. No.                         3.44\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here \n",
    "goodcolumns=['weathersit_2', 'holiday']\n",
    "new_features_const=features_const[goodcolumns] \n",
    "fit=sm.OLS(labels,new_features_const).fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.7 (3 points)**: Discuss the difference in fit quality between the two fits. Did the second fit (with \"least important\" feature removed) outperform or underperform compared to the other? Describe how you evaluated the quality based on the fit statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> it underpreformed as the r^2 value was lower as a r^2 value gets better the closer it is to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.8 (5 points)**: What does this all mean? What is your best R-squared value or adjusted R-squared value? What can we say about the data and how it might be related? What does it mean when people say that correlation is not causation? Please try to answer these questions the best that you can in a paragraph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The R^2 was 1 that could be due to errors in cleaning that I had however an R^2 of 1 shows strong positive correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 3**\", and push the changes to GitHub.\n",
    "\n",
    "\n",
    "If committing and/or pushing isn't working for you, write down the complete commands in this cell that would have committed your changes (with the commit message) and pushed them to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 4: Support vector machine (SVM) classification (34 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exam, we will use a support vector machine (SVM) classifier to identify emails as spam or not based on various email features. We will be using the UC Irvine Machine Learning Repository Spambase Dataset. More info about this dataset can be found here https://archive.ics.uci.edu/dataset/94/spambase. \n",
    "\n",
    "To get started, download the `spambasedata.csv` file from the link below (or D2L) and place it in the same directory as your notebook. \n",
    "\n",
    "`https://raw.githubusercontent.com/gambre11/CMSE202/refs/heads/main/spambasedata.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.1 (2 points)**: Read in the `spambase.csv` dataset into a `Pandas` `DataFrame` and display the first few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.671</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.450</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.022</td>\n",
       "      <td>9.744</td>\n",
       "      <td>445</td>\n",
       "      <td>1257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.729</td>\n",
       "      <td>43</td>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.312</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...  0.41  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.00   \n",
       "5  0.00  0.00    0.00  0.0  1.92  0.00  0.00  0.00  0.00  0.64  ...  0.00   \n",
       "6  0.00  0.00    0.00  0.0  1.88  0.00  0.00  1.88  0.00  0.00  ...  0.00   \n",
       "7  0.15  0.00    0.46  0.0  0.61  0.00  0.30  0.00  0.92  0.76  ...  0.00   \n",
       "8  0.06  0.12    0.77  0.0  0.19  0.32  0.38  0.00  0.06  0.00  ...  0.04   \n",
       "9  0.00  0.00    0.00  0.0  0.00  0.00  0.96  0.00  0.00  1.92  ...  0.00   \n",
       "\n",
       "    0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "5  0.054   0.0  0.164  0.054  0.000  1.671    4   112  1  \n",
       "6  0.206   0.0  0.000  0.000  0.000  2.450   11    49  1  \n",
       "7  0.271   0.0  0.181  0.203  0.022  9.744  445  1257  1  \n",
       "8  0.030   0.0  0.244  0.081  0.000  1.729   43   749  1  \n",
       "9  0.000   0.0  0.462  0.000  0.000  1.312    6    21  1  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code for Question 4.1 here\n",
    "bike_data= pd.read_csv('spambasedata.csv')\n",
    "bike_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.2 (2 points)**: The following cell contains a list with the names of every column in this dataset. Add these column names to your data that you have read in, and display your data again. Double check and make sure you are not losing any data when doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \n",
    "                      \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \n",
    "                      \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \n",
    "                      \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\", \n",
    "                      \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \n",
    "                      \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \n",
    "                      \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \n",
    "                      \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \n",
    "                      \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "                      \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \n",
    "                      \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \n",
    "                      \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_hash\", \"capital_run_length_average\", \n",
    "                      \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"] \n",
    "spamdata.columns = column_names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.3 (2 points)**: How many rows of data are there? How many emails are marked as spam and what is this percentage compared to the entire dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spam\n",
       "0    2788\n",
       "1    1812\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your work and answer to Question 4.3 here \n",
    "print(len(spamdata)) \n",
    "spamdata.head(10) \n",
    "spamdata['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.4 (2 points)**: Our goal is to classify the `spam` of the email given the other 57 features. Create a variable with all of the columns of the `DataFrame` except for `spam`,  and another variable with just the `spam` column of the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code for Question 4.4 here \n",
    "spam=spamdata[\"spam\"] \n",
    "nospam=spamdata.drop(\"spam\",axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is properly loaded into Python, we need to perform a **train-test-split** so that we can build our SVM classifier and test it.\n",
    "\n",
    "&#9989; **Question 4.5 (4 points)**: Use the `train_test_split()` method from `sklearn.model_selection` like we did in class. Use a `train_size` of `0.78` and `random_state` of `1616`. You should now have training features, testing features, training labels, and testing labels. Finally, **print the shape of your training features, training labels, testing features, and testing labels** to verify that your train-test-split did what it was supposed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1012,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code for Question 4.5 here \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(nospam,spam, test_size=0.22, random_state=1616)\n",
    "X_train.shape\n",
    "X_test.shape \n",
    "y_train.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.6 (6 points)**: Fit an SVM classifier (using the `sklearn` `SVC` class) to the dataset. Use a `linear` kernel and set the hyper-parameter to be `C=0.001.` Then **fit the SVM using your training set** and use the resulting SVM to **predict the labels for the testing set** so you get predicted labels for the testing set. Finally, **print the fit statistics** using the `confusion_matrix()` and `classification_report()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.85      0.93      0.89       606\\n           1       0.88      0.76      0.81       406\\n\\n    accuracy                           0.86      1012\\n   macro avg       0.87      0.84      0.85      1012\\nweighted avg       0.86      0.86      0.86      1012\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code for Question 4.6 here \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "svm = SVC(kernel='linear', C=0.001)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.7 (2 points)**: Create two more SVM classifiers and test them by repeating your work from Question 4.6, but this time set the hyper-parameters to be `C=0.01.` and `C=0.1.` Again, **print the fit statistics** using the `confusion_matrix()` and `classification_report()` methods. Note that this might take 30 seconds to a minute to run. If it takes longer than 2 minutes, flag an instructor for help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[575  31]\n",
      " [ 46 360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       606\n",
      "           1       0.92      0.89      0.90       406\n",
      "\n",
      "    accuracy                           0.92      1012\n",
      "   macro avg       0.92      0.92      0.92      1012\n",
      "weighted avg       0.92      0.92      0.92      1012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Put your code for Question 4.7 here\n",
    "svm = SVC(kernel='linear', C=0.01)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "classification_report(y_test, y_pred)\n",
    "\n",
    "svm = SVC(kernel='linear', C=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "matrix=confusion_matrix(y_test, y_pred)\n",
    "report=classification_report(y_test, y_pred) \n",
    "print(matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.8 (6 points)**: Interpret the outputs of your classification reports and the confusion matrices. In particular: \n",
    "\n",
    "* From your classification report, talk about what the **precision**, **recall**, and **f1-score** mean in this context. Do not just give definitions.\n",
    "\n",
    "* From your confusion matrix, what do each of the four numbers represent in this context. Again do not give a broad definition.\n",
    "\n",
    "* How does your accuracy change as C changes? Why does this happen? What is the parameter C and what does increasing or decreasing its value represent when classifying data using SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **the precision score shows that the model is accurate in predicting whether an email is spam or not the recall score shows that the model is good at being accurate c means model sensitivity ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "&#9989; **Question 4.9 (1 point)**: Suppose we wanted to try fitting a Support Vector Classifier for multiple choices of the kernel function and multiple choices for the values of the hyperparameter(s) (instead of just using a `linear` kernel with one value of `C`). We could write code with nested for loops to repeat the procedure with every combination of kernel function and hyperparameter value(s) we wanted to try. Name a method built into sklearn that will do this automatically. (We used this on an in-class assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **that method is gridsearch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "&#9989; **Question 4.10 (1 point)**: When filtering out spam emails, we want to minimize filtering out emails that may be important. What could we do to prioritize this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **adjust the decision threshold **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.11 (3 points)**: Both of the images below show the same two-dimensional dataset with two classes (solid blue squares and unfilled red circles) along with the decision boundary of a linear classifier. One classifier was generated via the Perceptron Learning Algorithm, and the other used a Support Vector Classifier. Which one is which? **Justify your answer!**\n",
    "\n",
    "Classifier A          | | Classifier B\n",
    ":-------------------------:|:---:|:-------------------------:\n",
    "![](https://i.ibb.co/R2BBsDC/Datapoints1-A.png)  | |  ![](https://i.ibb.co/mb9vcq4/Datapoints1-B.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier b is the svc as it is more accuarate over and the data is more seperated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **Do This - Erase the contents of this cell an put your answer here.** classifier y is c=1000 as it is less precise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.12 (3 points)**: Both of the images below show the same two-dimensional dataset with two classes (solid blue squares and unfilled red circles) along with the decision boundary of a linear Support Vector Classifier. One used the hyperparameter `C = 0.1`, and the other used the hyperparameter `C = 1000`. Which one is which? **Justify your answer!**\n",
    "\n",
    "Classifier X          | | Classifier Y\n",
    ":-------------------------:|:---:|:-------------------------:\n",
    "![](https://i.ibb.co/7pPCRwh/Datapoints2-A.png)  | |  ![](https://i.ibb.co/LSMBXzd/Datapoints2-B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 4**\", and push the changes to GitHub.\n",
    "\n",
    "\n",
    "If committing and/or pushing isn't working for you, write down the complete commands in this cell that would have committed your changes (with the commit message) and pushed them to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 5: Principal Component Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5 (5 points)**: What was the point of doing Principal Component Analysis on our face data from our day 23 ICA? What does Principal Component Analysis help us do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> ** it allows us to minimize dimensionality in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done! Congrats on finishing your CMSE 202 Final!\n",
    "\n",
    "Make sure all of your changes to your repository are committed and pushed to GitHub (or that you wrote down the commands that would have done that after each part). Also upload a copy of this notebook to the dropbox on D2L in case something went wrong with your repository or if you couldn't get the repository to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
